{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CONVOLUTION\n\nIn PyTorch, convolutions are implemented using the __nn.Conv1d, nn.Conv2d, or nn.Conv3d__ modules, depending on the dimensionality of your data. Convolutional layers are the backbone of many deep learning models, particularly in computer vision and time-series applications.\n\nA Convolution operation is a filter(kernel), to an input, e.g (image or a sequence), to produce a feature map. The filter slides across the input, performing  an element-wise multiplication and summation, effectively capturing the local patterns in the data.\n\n![2D Convolution Animation](https://upload.wikimedia.org/wikipedia/commons/1/19/2D_Convolution_Animation.gif)\n","metadata":{}},{"cell_type":"markdown","source":"## Parameters of CNN Layer\n\n1. in_channels\n2. out_channels\n3. kernel_size\n4. Stride\n5. padding\n6. dilation\n7. groups\n8. bias\n\n### In_channels\n\nIt specifies the number of input channels the layer expects. This is useful for the filters. They represent the dimensions of features of the input data. For an __RGB__ image, the channels are __Red, Green, Blue__ so __in_channels = 3__. For a __grayscale__ image, there is only one channel (intensity - how dark it is), so __in_channels = 1__. In audio or sequential data, channels might represent features like MFCCs, so in_channels depends on the number of extracted features.\n\n### Out_channels\n\nThe __out_channels__ parameter in a convolutional layer determines the number of output channels (or feature maps) that the layer produces after applying the convolution operation. This number normally represents how many different feature maps/filters you would want the convolution layer to learn.\n\nEach filter in the convolution layer learns a different set of features from the input, so increasing __out_channels__ increases the number of features the network learns. \n\n### Kernel Size\n\nIt determines the spatial dimensions of the filters (or kernels) applied to the input data. It defines the size of the window that slides over the input tensor to perform the convolution operation.\n\n__2D Convolutions (Images):__\n\nThe kernel is a 2D matrix that slides over the 2D input image. The size of the kernel is defined by two dimensions: height and width.\nCommon choices for kernel sizes are __3x3, 5x5,__ and __7x7__, though other sizes can also be used depending on the problem.\n\n__1D Convolutions (Sequences or Audio):__\n\nThe kernel is a 1D vector that slides over the 1D input sequence.\nThe kernel size is typically an odd number to ensure that there is a center element (like 3, 5, etc.).\n\nSmaller kernels (e.g., 3x3 or 5x5) capture local features in a fine-grained way, allowing the network to focus on small spatial details.\n\nLarger kernels (e.g., 7x7 or more) cover larger portions of the input and may capture more global features, but they also require more parameters and computation.\n\n### Stride\n\nIt refers to the step size, at which filters/kernels  move across the input data. It controls how much the filter shifts after each operation.\n\n- __Stride = 1:__ The filter moves one unit at a time across the input. A stride of 1 ensures that the filter slides by one pixel at a time, producing a more detailed output feature map with higher spatial resolution.\n- __Stride = 2:__ The filter moves two units at a time, effectively reducing the output size by a factor of 2 in that dimension. Larger strides can help in downsampling the input (reducing the size of the feature map), which is sometimes useful in reducing the computational load and capturing high-level features.\n\n\nYou can see that the stride affects the output size yeah? so this is how the output stride is calculated:\n\n\n### Convolution Output Size Formula\n\n$\n\\text{Output Size} = \\frac{\\text{Input Size} - \\text{Kernel Size} + 2 \\times \\text{Padding}}{\\text{Stride}} + 1\n$\n\n#### Components:\n- **Input Size**: Size of the input feature map.\n- **Kernel Size**: Size of the filter applied to the input.\n- **Padding**: Number of pixels added to the border of the input.\n- **Stride**: Step size for the filter's movement across the input.\n- **+1**: Ensures the first position of the kernel is counted.\n\n#### Example:\nFor a 2D convolution with:\n- Input Size = 32,\n- Kernel Size = 5,\n- Padding = 2,\n- Stride = 1,\n\nThe output size is:\n\n$\n\\text{Output Size} = \\frac{32 - 5 + 2 \\times 2}{1} + 1 = 32\n$\n\n### Padding\n\n__Padding__ is the process of adding extra elements around the input feature map to control the size of the output. It is commonly used to preserve the input's spatial dimensions or to achieve specific output sizes.\n\nFormula for Padding (Same Output Size):\n$\n\\text{Padding} = \\frac{\\text{Kernel Size} − 1}{2}\n$\n \nThis formula ensures the output size remains the same as the input size when the stride is 1. The padding is distributed evenly on both sides of the input.\n\n#### Types of Padding\n\n1. __Valid padding:__ No extra elements are added, the output size is reduced depending on the kernel size and stride using the formula shared in the convolution output formula\n2. __Same padding:__ Padding is added  to ensure the output size is equal to the input size, using the immediate above formula\n3. __Custom padding:__ users can define arbitrary padding values. For example, padding of 1 adds one extra element to all sides of the input.","metadata":{}},{"cell_type":"markdown","source":"Convolution layers in neural networks come in different variants tailored to the nature of the input data: __1D, 2D,__ and __3D.__\n\n### 1D\n\nA 1D convolution operates on sequential data along a single spatial axis. It slides a filter (or kernel) across the input, performing dot products between the filter and overlapping input segments. The result is a sequence of features that captures local patterns in the input data.\n\n![1D convolution](https://e2eml.school/images/conv1d/aa_copy.gif)","metadata":{}},{"cell_type":"code","source":"conv1d = nn.Conv1d(in_channels = 1, out_channels = 4, kernel_size = 3, stride = 1, padding = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T10:32:58.264765Z","iopub.execute_input":"2024-12-26T10:32:58.265149Z","iopub.status.idle":"2024-12-26T10:32:58.271088Z","shell.execute_reply.started":"2024-12-26T10:32:58.265117Z","shell.execute_reply":"2024-12-26T10:32:58.269691Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### 2D\n\nThis operates in a 2D data, i.e images, it slides a 2D filter/kernel, across the input image, performing dot products between the filter and overlapping regions of the images. \n\n![convolution 2D](https://miro.medium.com/v2/resize:fit:828/format:webp/1*DTTpGlhwkctlv9CYannVsw.gif)","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nconv2d = nn.Conv2d(in_channels = 3,     out_channels = 8, kernel_size = 3, stride = 1, padding = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T10:32:00.044633Z","iopub.execute_input":"2024-12-26T10:32:00.045197Z","iopub.status.idle":"2024-12-26T10:32:00.051640Z","shell.execute_reply.started":"2024-12-26T10:32:00.045161Z","shell.execute_reply":"2024-12-26T10:32:00.050470Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### 3D\n\nThis operates on volumentric data, such as  video frames (time, height, width), medical imaging data or any data with three spatial dimensions (depth, height, width). Instead of sliding a 2D kernel, a 3D kernel moves through the data in three dimensions, capturing spatial and temporal relationships.\n\n[![Convolution 3D Video](https://i.imgur.com/2nJzE83.jpg)](https://imgur.com/2nJzE83)\n\n\n#### Applications of 3D Convolution\n\n- __Video Analysis:__ Action recognition, video classification, and object tracking.\n- __Medical Imaging:__ Processing 3D scans (e.g., CT, MRI).\n- __Scientific Computing:__ Analyzing volumetric data in simulations.\n- __Speech Processing:__ Features from spectrograms with time, frequency, and channel dimensions.\n\n\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nconv3d = nn.Conv3d(in_channels = 3, out_channels = 8, kernel_size = (3,3,3), stride = 1, padding = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T10:30:43.365329Z","iopub.execute_input":"2024-12-26T10:30:43.366297Z","iopub.status.idle":"2024-12-26T10:30:48.086981Z","shell.execute_reply.started":"2024-12-26T10:30:43.366144Z","shell.execute_reply":"2024-12-26T10:30:48.085623Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Patterns formed when CNN Architecture\n\nThe general architecture formed is `Conv layer -> Batch norm -> Activation`. This pattern repeats itself across the entire CNN architecture.\n\n### Conv layer\n\nThis is what learns the edges, vertices, the features -  it extracts the raw spatial features\n\n### Batch Norm\n\nIt normalizes the output of the conv layer, so that it has a mean of 0 and a variance of 1. This is stabilizes the output of the convolution layer\n\n### Activation layer\n\ndifinitely to introduce non-linearity","metadata":{}},{"cell_type":"code","source":"class CNNArchitecture(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n        super(CNNArchitecture, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels,kernel_size, stride, padding)\n        self.batchnorm = nn.BatchNorm2d(out_channels)\n        self.act = nn.ReLU()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.batchnorm(x)\n        x = self.act(x)\n\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The same pattern repeats itself even in the resnet block architecture","metadata":{}},{"cell_type":"code","source":"class BasicCNNBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, downsample = 0):\n        super(BasicCNNBlock, self).__init__()\n        self.conv1 = nn.Conv2D(in_channels, out_channels, kernel_size=3, stride = stride, padding = padding)\n        self.batchnorm1 = nn.BatchNorm2d(out_channels)\n        self.act1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2D(in_channels, out_channels, kernel_size=3, stride = stride, padding = padding)\n        self.batchnorm2 = nn.BatchNorm2d(out_channels)\n        self.downsample = downsample\n\n    def forward(self, x):\n        identity = x\n        x = self.conv1(x)\n        x = self.batchnorm1(x)\n        x = self.act1(x)\n        x = self.conv2(x)\n        x = self.batchnorm2(x)\n        if self.downsample is not None:\n            identity = self.downsample(x)\n        x += identity\n\n        return self.relu(x)\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
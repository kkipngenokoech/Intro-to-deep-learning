{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CONVOLUTION\n\nIn PyTorch, convolutions are implemented using the __nn.Conv1d, nn.Conv2d, or nn.Conv3d__ modules, depending on the dimensionality of your data. Convolutional layers are the backbone of many deep learning models, particularly in computer vision and time-series applications.\n\nA Convolution operation is a filter(kernel), to an input, e.g (image or a sequence), to produce a feature map. The filter slides across the input, performing  an element-wise multiplication and summation, effectively capturing the local patterns in the data.\n\n![2D Convolution Animation](https://upload.wikimedia.org/wikipedia/commons/1/19/2D_Convolution_Animation.gif)\n","metadata":{}},{"cell_type":"markdown","source":"## Parameters of CNN Layer\n\n1. in_channels\n2. out_channels\n3. kernel_size\n4. Stride\n5. padding\n6. dilation\n7. groups\n8. bias\n\n### In_channels\n\nIt specifies the number of input channels the layer expects. This is useful for the filters. They represent the dimensions of features of the input data. For an __RGB__ image, the channels are __Red, Green, Blue__ so __in_channels = 3__. For a __grayscale__ image, there is only one channel (intensity - how dark it is), so __in_channels = 1__. In audio or sequential data, channels might represent features like MFCCs, so in_channels depends on the number of extracted features.\n\n### Out_channels\n\nThe __out_channels__ parameter in a convolutional layer determines the number of output channels (or feature maps) that the layer produces after applying the convolution operation. This number normally represents how many different feature maps/filters you would want the convolution layer to learn.\n\nEach filter in the convolution layer learns a different set of features from the input, so increasing __out_channels__ increases the number of features the network learns. \n\n### Kernel Size\n\nIt determines the spatial dimensions of the filters (or kernels) applied to the input data. It defines the size of the window that slides over the input tensor to perform the convolution operation.\n\n__2D Convolutions (Images):__\n\nThe kernel is a 2D matrix that slides over the 2D input image. The size of the kernel is defined by two dimensions: height and width.\nCommon choices for kernel sizes are __3x3, 5x5,__ and __7x7__, though other sizes can also be used depending on the problem.\n\n__1D Convolutions (Sequences or Audio):__\n\nThe kernel is a 1D vector that slides over the 1D input sequence.\nThe kernel size is typically an odd number to ensure that there is a center element (like 3, 5, etc.).\n\nSmaller kernels (e.g., 3x3 or 5x5) capture local features in a fine-grained way, allowing the network to focus on small spatial details.\n\nLarger kernels (e.g., 7x7 or more) cover larger portions of the input and may capture more global features, but they also require more parameters and computation.\n\n### Stride\n\nIt refers to the step size, at which filters/kernels  move across the input data. It controls how much the filter shifts after each operation.\n\n- __Stride = 1:__ The filter moves one unit at a time across the input. A stride of 1 ensures that the filter slides by one pixel at a time, producing a more detailed output feature map with higher spatial resolution.\n- __Stride = 2:__ The filter moves two units at a time, effectively reducing the output size by a factor of 2 in that dimension. Larger strides can help in downsampling the input (reducing the size of the feature map), which is sometimes useful in reducing the computational load and capturing high-level features.\n\n\nYou can see that the stride affects the output size yeah? so this is how the output stride is calculated:\n\n\n### Convolution Output Size Formula\n\n$\n\\text{Output Size} = \\frac{\\text{Input Size} - \\text{Kernel Size} + 2 \\times \\text{Padding}}{\\text{Stride}} + 1\n$\n\n#### Components:\n- **Input Size**: Size of the input feature map.\n- **Kernel Size**: Size of the filter applied to the input.\n- **Padding**: Number of pixels added to the border of the input.\n- **Stride**: Step size for the filter's movement across the input.\n- **+1**: Ensures the first position of the kernel is counted.\n\n#### Example:\nFor a 2D convolution with:\n- Input Size = 32,\n- Kernel Size = 5,\n- Padding = 2,\n- Stride = 1,\n\nThe output size is:\n\n$\n\\text{Output Size} = \\frac{32 - 5 + 2 \\times 2}{1} + 1 = 32\n$\n\n### Padding\n\n__Padding__ is the process of adding extra elements around the input feature map to control the size of the output. It is commonly used to preserve the input's spatial dimensions or to achieve specific output sizes.\n\nFormula for Padding (Same Output Size):\n$\n\\text{Padding} = \\frac{\\text{Kernel Size} − 1}{2}\n$\n \nThis formula ensures the output size remains the same as the input size when the stride is 1. The padding is distributed evenly on both sides of the input.\n\n#### Types of Padding\n\n1. __Valid padding:__ No extra elements are added, the output size is reduced depending on the kernel size and stride using the formula shared in the convolution output formula\n2. __Same padding:__ Padding is added  to ensure the output size is equal to the input size, using the immediate above formula\n3. __Custom padding:__ users can define arbitrary padding values. For example, padding of 1 adds one extra element to all sides of the input.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
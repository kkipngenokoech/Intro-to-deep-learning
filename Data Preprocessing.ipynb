{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DATA","metadata":{}},{"cell_type":"markdown","source":"## Sentimental Analysis","metadata":{}},{"cell_type":"markdown","source":"### Raw Data\nSuppose we are building a sentiment analysis model. Here are our sentences:\n\n1. \"I love PyTorch.\"\n2. \"The movie was terrible.\"\n3. \"PyTorch makes deep learning fun!\"\n4. \"I didn't like the food.\"\n\nEach sentence has a corresponding label:\n\n- __1__ for positive sentiment.\n- __0__ for negative sentiment.\n\nSo the labels are: __[1,0,1,0]__\n\n### Tokenization\nWe need to tokenize these sentences, let us split it into words:\n\n1. Sentence 1: [\"I\", \"love\", \"PyTorch\"]\n2. Sentence 2: [\"The\", \"movie\", \"was\", \"terrible\"]\n3. Sentence 3: [\"PyTorch\", \"makes\", \"deep\", \"learning\", \"fun\"]\n4. Sentence 4: [\"I\", \"didn't\", \"like\", \"the\", \"food\"]\n\n\n### Creating Vocab\n\nFrom the above sentences we can extract unique words, and match them into integers so that we create something like a lookup table:\n\n```python\nVocabulary:\n{\"I\": 1, \"love\": 2, \"PyTorch\": 3, \"The\": 4, \"movie\": 5, \"was\": 6, \n \"terrible\": 7, \"makes\": 8, \"deep\": 9, \"learning\": 10, \"fun\": 11, \n \"didn't\": 12, \"like\": 13, \"the\": 14, \"food\": 15}\n```\n\nYou notice from above, we have pytorch & I appearing twice but we assign only a vocab index once?\n\n### Converting our tokenized sentences into Sequence of Numbers\n\n1. Sentence 1: [1, 2, 3]\n2. Sentence 2: [4, 5, 6, 7]\n3. Sentence 3: [3, 8, 9, 10, 11]\n4. Sentence 4: [1, 12, 13, 14, 15]\n\nWhat you notice is that these sentences have different lengths, and to process data in batches, they are supposed to be of the same lengths, we are going to pad the shorter texts to match the longest one in the batch.\n\nPadded Sequences:\n1. Sentence 1: [1, 2, 3, 0, 0]\n2. Sentence 2: [4, 5, 6, 7, 0]\n3. Sentence 3: [3, 8, 9, 10, 11]\n4. Sentence 4: [1, 12, 13, 14, 15]\n\n### Creating batches\n\nNow we are going to have a batch size of 2, that means we will be sending two sentences per batch:\n\n#### Batch 1\n\n```python\nInput = [[1, 2, 3, 0, 0], [4, 5, 6, 7, 0]]\nLabels = [1, 0]\n```\n\n#### Batch 2\n```python\nInput =  [[3, 8, 9, 10, 11], \n        [1, 12, 13, 14, 15]]\nLabels = [1, 0]\n```\n\nFrom the above, you can see that the shape of each batch is 2 by 5. `[batch_size, sequence_length]`","metadata":{}}]}
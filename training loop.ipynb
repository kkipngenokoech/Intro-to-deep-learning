{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TRAINING LOOP\n\nThe training loop is a crucial part of training a machine learning model. It iterates over the data, computes predictions, compares them to ground truth using a loss function, and updates model parameters through backpropagation.\n\nWe normally take a batch from our training set on every iteration of the loop, which is handled by the dataloader. We then run these batches through our model, and compute the loss from the expected output. We then compute the gradient using optimizer to adjust the weights.","metadata":{}},{"cell_type":"markdown","source":"## prerequistes\n\nYou need a model, a loss, a dataloader and an optimizer.\n\n### Model\n\nThe model is used to make predictions based on the input data, these predictions are then compared to the ground truth to calculate the loss.\n\n### Loss\n\nThe loss function/cost function/Objective function, computes how far the model's predictions are from the true labels.\n\n### DataLoader\n\nIt is used to load the dataset in batches, allowing for efficient processing of data\n\n### Optimizer\n\nThe optimizer is used to adjust the model's weights based on the gradients computed during the backprop process: `adam, SGD, RMSProp`.\n\n### learning rate scheduling (optional param)\n\nLearning rate schedulers adjust the learning rate during training based on predefined rules (e.g., reducing the learning rate after certain epochs).","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import datasets\n\n# Dataset\ntrain_dataset = datasets.CIFAR10(\n    download = True,\n    shuffle = True,\n    root = \"data\",\n    train = True,\n)\n\n\n## Data loader\n\ntrain_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n\n\n## Model definition\nmodel = nn.Sequential(\n    nn.Linear(10, 50),\n    nn.ReLU(),\n    nn.Linear(50, 1)\n)\n\n# Loss\ncriterion = nn.MSELoss()\n\n# Optimizer\noptimizer = nn.optim.SGD(model.parameters(), lr = 0.01)\n\n# Scheduler stepping\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### training loop\n\nNow, in the training loop, we bring all of the above prerequistes into one loop/function\n","metadata":{}},{"cell_type":"code","source":"num_epochs = 100\nfor epoch in range(num_epochs):\n    model.train() # sets the model into training mode\n    for inputs, labels in train_loader:\n        # Forward pass\n        predictions  = model(inputs)\n        loss = criterion(predictions, labels)\n\n        # Backward pass (backprop)\n        optimizer.zero_grad() # zeros out the prev gradients, so that updates are only based on current gradients for this particular batch, not accumulated gradients \n        loss.backward() # it is used to compute the gradients via back prop\n        optimizer.step() # updates the model params using gradients computed in the loss.backward()\n\n    # Optional lr scheduling\n    scheduler.step()\n        \n    \n    # End of an epoch\n    print(f\"Epoch {epoch+1}, loss: {loss.item()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
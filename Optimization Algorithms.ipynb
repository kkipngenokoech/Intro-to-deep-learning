{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IMPORTING NECESSARY LIBRARIES","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch.nn as nn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T17:32:27.833259Z","iopub.execute_input":"2024-12-17T17:32:27.834394Z","iopub.status.idle":"2024-12-17T17:32:31.127314Z","shell.execute_reply.started":"2024-12-17T17:32:27.834335Z","shell.execute_reply":"2024-12-17T17:32:31.125901Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# OPTIMIZATION ALGORITHMS\n\nThey are used to minimize or maximize a loss function by adjusting model parameters during training. In pytorch they are held under `torch.optim`\n\nThey include:\n\n1. Gradient descent\n2. Momentum\n3. Nestrov accelerated Gradient (NAG)\n4. Adagrad\n5. RMSProp\n6. Adam (Adaptive moment Estimation\n7. AdamW - Weight decay Adam","metadata":{}},{"cell_type":"markdown","source":"## Gradient descent\n\nAdjust parameters by computing the gradient of the loss function. They include:\n\n1. Batch Gradient descent (the entire batch)\n2. Stochastic gradient descent (one sample per time)\n3. mini-batch gradient descent (a sample of the batch)\n\nThe formula to update the gradient descent : ***param = param - learning rate * Gradient of the loss function w.r.t to tetha***","metadata":{}},{"cell_type":"code","source":"class SGD:\n    def __init__(self,model, learning_rate = 0.01):\n        self.model = model\n        self.lr = learning_rate\n    def step(Self):\n        for param, grad in self.model.gradients().items():\n            # self.model.gradients() - returns a dictionary of computed gradients for each parameter (param) in the model\n            # updating the params\n            self.model.parameters()[param] -= self.lr * grad\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In `torch.optim`, we import SGD: `torch.optim.SGD` to perform stochastic gradient descent automatically using computed gradients\n```python\noptimizer = nn.optim.SGD(model.parameters(), lr = 0.01) # this is defined outside the train loop\noptimizer.step() # this is defined inside the train loop\n```\npytorch will automatically update the model's parameters. You just call `optimizer.step()` & this is called immediately after `loss.backward()`. Remember before you need to clear out previous gradients to avoid it accumulating: `optimizer.zero_grad()`\n\nAll this is done in the train by the way, After every batch loaded into the train method\n\nParams that could be passed into the SGD:\n\n1. required\n   - **model.parameters()** - these are the learnable params from the model (normally the weights and the biases) : these are the things you keep updating until you find the optimal solutions\n   - **lr** - learning rate that controls the step size for updating parameters\n3. optional\n   - **momentum** : this is used to add a fraction of the previous gradient to the current one to accelerate convergence\n   - **weight decay** : Adds L2 Regularization (also ridge regression) to prevent overfitting\n   - **dampening** : dampens the momentum effect\n   - **nestrov** : enables nestrov's momentum, which looks ahead by applying momentum before the gradient update","metadata":{}},{"cell_type":"markdown","source":"## Momentum\n\nAccelerates gradient descent by accumulating a velocity vector that considers past gradients\n\nThis is an enhanced SGD that helps accelerates convergence by using a moving average of past gradients. It maintains a velocity `V_t`. Parameters are updated using the velocity. \n\nSo we first compute the velocity: `V_t = (momentum factor)(prev velocity) + (1 - momentum factor)(current gradient)`. Then you use the velocity to update the parameters: `param -= lr*V_t`. This is accelerates convergence.\n\n**Lower momentum** reduces the influence of past gradients, making updates more senstive to new gradients while **higher momentum** increases the influence of past gradients, smoothening updates but risking overfitting","metadata":{}},{"cell_type":"markdown","source":"With momentum:\n\n1. convergence speed **Faster**\n2. Oscilations: **Lower**\n3. Escaping local minima: **Easy**\n4. Stability: **More stable**","metadata":{}},{"cell_type":"markdown","source":"In pytorch, momentum is an optional param in `torch.nn.optim.SGD`. \n```py\noptimizer = torch.nn.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9)\n```\n\nAgain in the training loop, remember:\n```py\noptimizer.zero_grad()\n# perform forward pass (output = model(inputs), loss = criterion(outputs, targets))\n# perform backward pass (loss.backward())\n# perform weights updates\noptimizer.step()\n# repeat the cycle\n\n```","metadata":{}},{"cell_type":"markdown","source":"## Nestrov Accelerated Gradient (NAG)\n\nSimilar to mometum but looks ahead to calculate the gradient","metadata":{}},{"cell_type":"markdown","source":"## Adagrad\n\nAdjusts the learning rate for each parameter based on past gradients","metadata":{}},{"cell_type":"markdown","source":"## RMSProp\n\nFixed Adagrad's decaying learning rate problem by using a moving average of squared gradients","metadata":{}},{"cell_type":"markdown","source":"## Adam (Adaptive moment Estimation)\n\ncombines momentum and RMSProp for adaptive learning rates","metadata":{}},{"cell_type":"markdown","source":"## AdamW (Weight decay Adam)\n\nVariant of Adam that decouples weight decays from the optimization step","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
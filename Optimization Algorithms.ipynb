{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IMPORTING NECESSARY LIBRARIES","metadata":{}},{"cell_type":"code","source":"import numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T17:07:42.488400Z","iopub.execute_input":"2024-12-17T17:07:42.488872Z","iopub.status.idle":"2024-12-17T17:07:42.521526Z","shell.execute_reply.started":"2024-12-17T17:07:42.488831Z","shell.execute_reply":"2024-12-17T17:07:42.520283Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# OPTIMIZATION ALGORITHMS\n\nThey are used to minimize or maximize a loss function by adjusting model parameters during training.\n\nThey include:\n\n1. Gradient descent\n2. Momentum\n3. Nestrov accelerated Gradient (NAG)\n4. Adagrad\n5. RMSProp\n6. Adam (Adaptive moment Estimation\n7. AdamW - Weight decay Adam","metadata":{}},{"cell_type":"markdown","source":"## Gradient descent\n\nAdjust parameters by computing the gradient of the loss function. They include:\n\n1. Batch Gradient descent (the entire batch)\n2. Stochastic gradient descent (one sample per time)\n3. mini-batch gradient descent (a sample of the batch)\n\nThe formula to update the gradient descent : ***param = param - learning rate * Gradient of the loss function w.r.t to tetha***","metadata":{}},{"cell_type":"code","source":"class SGD:\n    def __init__(self,model, learning_rate = 0.01):\n        self.model = model\n        self.lr = learning_rate\n    def step(Self):\n        for param, grad in self.model.gradients().items():\n            # self.model.gradients() - returns a dictionary of computed gradients for each parameter (param) in the model\n            # updating the params\n            self.model.parameters()[param] -= self.lr * grad\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Momentum\n\nAccelerates gradient descent by accumulating a velocity vector that considers past gradients","metadata":{}},{"cell_type":"markdown","source":"## Nestrov Accelerated Gradient (NAG)\n\nSimilar to mometum but looks ahead to calculate the gradient","metadata":{}},{"cell_type":"markdown","source":"## Adagrad\n\nAdjusts the learning rate for each parameter based on past gradients","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## RMSProp\n\nFixed Adagrad's decaying learning rate problem by using a moving average of squared gradients","metadata":{}},{"cell_type":"markdown","source":"## Adam (Adaptive moment Estimation)\n\ncombines momentum and RMSProp for adaptive learning rates","metadata":{}},{"cell_type":"markdown","source":"## AdamW (Weight decay Adam)\n\nVariant of Adam that decouples weight decays from the optimization step","metadata":{}}]}
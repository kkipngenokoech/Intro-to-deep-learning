{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LAYERS\n\n1. Linear layer\n2. convolution layer\n3. pooling layers (maxpool and average pooling)\n4. recurrent layers\n5. LSTM\n6. GRU\n7. Normalization layers\n8. Dropout layers\n9. Activation layers\n10. Embedding layers\n11. Attention mechanisms","metadata":{}},{"cell_type":"markdown","source":"## Linear Layer\n\nperforms a fully connected (dense) transformation. The equation for this layer is: __output = xW^T + b__ where:\n\nx = input vector\nW^T = weight matrix transponsed (this is what is learned), it maps input features to output features\nb = bias vector\n\nin pytorch `nn.Linear`:\n\n```python\nimport torch.nn as nn\nlinear = nn.Linear(in_features = 128, out_features = 64)\n# or\nlinear = nn.Linear(128, 64)\n```\n\nThe input tensor is normally _[batch_size, input_features]_ where batch size is the number of samples processed in a single forward pass. The weight matrix has the shape _[out_features, in_features]_ and it is initialized randomly by default. The model will learn appopriate values of these weights during training. The bias vector has the shape _[out_features]_. This is also updated during training.","metadata":{}},{"cell_type":"markdown","source":"## Convolution Layer\n\nA convolution layer is in the heart of convolution neural networks. its goal is to extract meaningful features from input data by applying convolutio operations. These layers performs a critical mathematical operation known as a __convolution__. \n\nThis process using filters known as __kernel__ that traverse through the input image to learn complex visual patterns.\n\n### convolution operation\n\nThis is a linear operation between two functions commonly applied to signals images or any structured data.\n\n![2D Convolution Animation](https://upload.wikimedia.org/wikipedia/commons/1/19/2D_Convolution_Animation.gif)\n\nwhen you have the input matrix, and the kernel, you overlay the kernel on the input matriz, do an elementwise multiplication (multiply each overlapping element), and then sum up the results it forms the first element in the output matrix, the slide the kernel based on the stride and and repeat until you get an output matrix.\n\nThis output matrix is known as a __feature map__.","metadata":{}},{"cell_type":"markdown","source":"### channels\n\nEach channel represents a color, each pixel consists of three channels if it is an RGB. An RGB image can be described as _W x H x c_. Gray scale images have only one channel. ","metadata":{}},{"cell_type":"markdown","source":"### Parameters\n\n1. Filter/Kernel size\n2. Stride\n3. Padding\n4. Output channels\n5. in channels\n\n```python\nconvLayer = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2, stride = 1, padding = 0)\n```\n\n#### Filter/kernel size\n\nThese are the learnable weight matrices. Each kernel extracts specific features like edges textures or patterns. for a filter size _k x K_ and input with c_in channels, total weights equates to _k x k x c_in_\n\n#### input channels\n\nrepresents the number of channels in the input data. for RGB it is three channels (Red, Green, Blue) and for grayscale it is just 1.\n\n#### Kernel size\n\nspatial size of the filter. Typically 3 x 3, 5 x 5, 7 x 7.\n\n#### Stride\n\ndetermines how much a filter move at each step. a stride of 1 means one pixel at a time. Larger strides reduces the size of the output feature map.\n\n#### padding\n\nadds zeros around the inputs to maintain its size after convolutions. \n\n#### Hyperparameters affecting parameters\n\n1. increasing output channels increases the number of filters and hence more learnable parameters\n2. increasing kernel size makes each filter larger\n3. ","metadata":{}},{"cell_type":"markdown","source":"## Pooling layers\n\nPooling, also known as subsampling or downsampling, is a technique used in CNNs to reduce the spatial dimensions of feature maps while retaining essential information\n\n1. MaxPooling\n2. AvePooling\n\n### MaxPooling\n\nTake the maximum value from each region of the feature map. It captures the most prominent feature in each region. \n\n### average pooling\n\nTake the average value from each region of the feature map\n\n### parameters\n\n1. __kernel size__  - determines the region to be pooled\n2. __stride__ - determines the step size of the pooling window, default is equal to the kernel size to ensure no overlapping regions\n3. __padding__ - add zeros around the input to control the output dimensions.\n\n```python\nmax_pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\navg_pool = nn.AvgPool2d(kernel_size = 2, stride = 2)\n```\n\npooling:\n\n1. reduces dimensions\n2. translation invariance - help the network recognize patterns regardless of small shifts input\n3. prevent overfitting","metadata":{}},{"cell_type":"markdown","source":"## Recurrent layers","metadata":{}},{"cell_type":"markdown","source":"A class of neural networks designed to process sequential data i.e audio, text and videos. It can use internal state (memory) to process sequences of inputs, making them ideal for tasks where context or temporal relationship matters.\n\nRNNs are powerful because:\n\n1. They can maintain context - retaining information about previous elements in the sequence\n2. Temporal dependency modelling - understanding the order and dependencies between elements\n\n### Sequential data\n\nArrangement of elements affect the outcome (so order matters). Elements in the sequence have relationships, a word's meaning depends on the preceeding words (context). These sequences also have variable lengths, some sentences are longer others are shorter\n\nImagine you are processing a sentence like: \"I love AI\", the sequence here would be \"__I__\" \"__love__\" \"__AI__\". at time step __t = 1__, the RNN takes in \"__I__\" and the initial hidden state $h_0$ is all zeros. Here we then compute $h_1 = f(input, h_0)$. At time step __t = 2__: it takes the second input = \"__love__\" and the hidden state $h_1$, it then computes $h_2 = f(input, h_1)$. At time step __t = 3__: it takes in the input __\"AI\"__ and the hidden state $h_2$ and computes $h_3 = f(input, h_2)$.\n\nAt each timestep, the RNN produces two key outputs:\n\n1. Hidden states of course, from the above paragraph\n2. The output -  processed results at the current timestep\n\n\n#### Hidden states formula\n\nThe hidden state at time step \\( t \\) is computed as:\n\n$$\nh_t = f(W_x x_t + W_h h_{t-1} + b_h)\n$$\n\nwhere:\n- \\( $h_t$ \\): Hidden state at time \\(t\\),\n- \\( f \\): Activation function (e.g., \\(\\tanh\\), ReLU),\n- \\( $W_x$ \\): Weight matrix for the current input \\($x_t$\\),\n- \\( $W_h$ \\): Weight matrix for the previous hidden state \\($h_{t-1}$\\),\n- \\( $b_h$ \\): Bias term.\n\n\n#### Output states formula\n\nThe output at time step \\( t \\) is computed as:\n\n$$\ny_t = g(W_y h_t + b_y)\n$$\n\nwhere:\n- \\( $y_t$ \\): Output at time \\(t\\),\n- \\( g \\): Output activation function (e.g., softmax, linear),\n- \\( $W_y$ \\): Weight matrix for mapping the hidden state to the output,\n- \\( $h_t$ \\): Hidden state at time \\(t\\),\n- \\( $b_y$ \\): Bias term for the output.\n\nThese formulas are repeated at each timestep until we get to the last time step\n\n\n","metadata":{}},{"cell_type":"markdown","source":"#### BPTT (Back Propagation Through Time)\n\nTo back propagate through these equations, we need to find the gradients w.r.t to the parameters $W_x, W_h, W_y, b_h, b_y$ using the chain rule of differentiation.\n\nHere are the forward equations:\n\nHidden state equation: $h_t = f(W_xx_t + W_hh_{t-1} + b_h)$\n\noutput equation: $y_t = g(W_yh_t + b_y)$\n\nThe loss function is defined as:\n\n$$\n\\mathcal{L} = \\sum_{t=1}^T \\ell(y_t, \\hat{y}_t)\n$$\n\nwhere:\n- \\( $\\mathcal{L}$ \\): Total loss across all time steps,\n- \\( $\\ell(y_t, \\hat{y}_t)$ \\): Loss at time \\( t \\) (e.g., cross-entropy or mean squared error),\n- \\( $y_t$ \\): Output at time \\( t \\),\n- \\( $\\hat{y}_t $\\): Ground truth at time \\( t \\),\n- \\( $T $\\): Total sequence length.\n\n\nFor each timestep, we calculate the output $y_t$ and the hidden output $h_t$. We need to compute the loss at each timestep (that is why we produce an output $y_t$. You compare this with ground truth $\\hat{y}_t$ then use a predefined loss function i.e cross entropy or mean squared error to find the loss. You then aggregate the total loss over the entire sequence by summing up this individual losses.\n\nThe reason why we sum up these losses across all the timesteps is because we want to ensure they all contribute to optimization. \n\nThen during back propagation, we take the derivative of this aggregated loss  w.r.t to outputs per timestep:\nFor each \\( t \\), you compute:  \n$\n\\frac{\\partial \\mathcal{L}}{\\partial y_t}\n$\n\nFrom here now, you can do Back propgation through time. start by calculating gradients w.r.t to hidden states:\n\nHere is the forward equation for the hidden state:\n$ h_t = f(W_xx_t + W_hh_{t-1} + b_h)$\n\nThe parameters we need to adjust is the $ W_x, W_h  \\text{ \\& }  b_h$\n\nThe gradient calcuated at each timestep was: $ \\frac{\\partial \\mathcal{L}}{\\partial y_t}  $ \n\nSo first we need to find the gradient w.r.t to $ h_t $ the hidden state in the final timestep. $ y_t = g(W_yh_t + b_y) $ ;\n\n$\\frac{\\partial \\mathcal{L}}{\\partial h_t} = \\frac{ \\partial \\mathcal{L}}{\\partial y_t} \\times \\frac{ \\partial y_t }{ \\partial h_t }$\n\nThen recursively compute the gradients w.r.t to previous timesteps (the forward equation for hidden states is always: $ h_t = f(W_xx_t + W_hh_{t-1} + b_h) $\n\nThe gradient w.r.t to the previous timestep  = $ \\frac {\\partial \\mathcal{L}}{\\partial y_t} \\times \\frac{\\partial y_t}{\\partial h_t} \\times \\frac{\\partial h_t}{\\partial h_{t-1}} \\cdots \\frac{\\partial h_1}{\\partial h_0} $\n\nOnce you have computed gradients w.r.t to each hidden states, You compute the gradients w.r.t to these parameters: $ W_x, W_h, b_h $ then you update them with whatever chosen optimizer i.e Gradient descent:\n\n$\nw \\gets w - \\eta \\frac{\\partial \\mathcal{L}}{\\partial w}\n$\n\n$\nb \\gets b - \\eta \\frac{\\partial \\mathcal{L}}{\\partial b}\n$","metadata":{}},{"cell_type":"markdown","source":"#### updating the weights w.r.t to outputs\n\nRemember there is a step above we calculated the gradients across the individual outputs? we then use that to update the weights and bias of each individual weights and biases for the output equation.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass SimpleRNN(nn.Module):\n    def __init(self, input_size, hidden_size, output_size):\n        super(SimpleRNN, self).__init__()\n\n        # RNN layer\n        self.rnn = nn.RNN(input_size, hidden_size, batch_first = True)\n        self.fc = nn.Linear(hidden_size, output_size)\n    def forward(self, x):\n        # out = output of all the time steps, _ is the hidden state of the last hidden state, back prop happens automatically, so we discard it since we don't need to use it\n        out, _ = self.rnn(x)\n\n        # let us extract the last output\n        out = self.fc(out[:,-1,:])\n        return out\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T11:42:50.246373Z","iopub.execute_input":"2024-12-25T11:42:50.250235Z","iopub.status.idle":"2024-12-25T11:42:54.704973Z","shell.execute_reply.started":"2024-12-25T11:42:50.250143Z","shell.execute_reply":"2024-12-25T11:42:54.703722Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## LSTM\n\n","metadata":{}},{"cell_type":"markdown","source":"Long term short term memory is a special type of Recurrent Neural Network designed to address the key limitations of Vanilla RNNs, particularly in handling long-term dependencies and mitigating issues like the vanishing gradient problem.\n\nHere's a breakdown of how LSTMs relate to RNNs and why LSTMs are often preferred over basic RNNs in many tasks:\n\n1. __Vanilla RNNs (Recurrent Neural Networks)__\nBasic Structure: A basic RNN consists of a loop in the hidden layer, which allows the network to maintain a state or memory over time. The network processes input sequences one element at a time, updating its hidden state at each time step.\n\n__Limitations:__\n\n - __Vanishing Gradient Problem:__ When training vanilla RNNs with backpropagation through time (BPTT), gradients can exponentially decay as they are propagated backward through many layers or time steps. This makes it hard to capture long-term dependencies in data, as the network forgets earlier information during training.\n\n- __Difficulty with Long-Term Dependencies:__ RNNs struggle to maintain information over long sequences because the information from earlier time steps gets \"forgotten\" or \"diluted\" as it propagates through many time steps.\n\n2. __Long Short-Term Memory (LSTM)__\nLSTM is a type of RNN, but it improves on the basic RNN by introducing gates and a cell state that help regulate the flow of information and enable the network to \"remember\" useful information over longer sequences.","metadata":{}},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"markdown","source":"### RNN ARCHITECTURE\n\n","metadata":{}}]}
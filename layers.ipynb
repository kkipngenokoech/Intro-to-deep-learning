{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LAYERS\n\n1. Linear layer\n2. convolution layer\n3. pooling layers (maxpool and average pooling)\n4. recurrent layers\n5. LSTM\n6. GRU\n7. Normalization layers\n8. Dropout layers\n9. Activation layers\n10. Embedding layers\n11. Attention mechanisms","metadata":{}},{"cell_type":"markdown","source":"## Linear Layer\n\nperforms a fully connected (dense) transformation. The equation for this layer is: __output = xW^T + b__ where:\n\nx = input vector\nW^T = weight matrix transponsed (this is what is learned), it maps input features to output features\nb = bias vector\n\nin pytorch `nn.Linear`:\n\n```python\nimport torch.nn as nn\nlinear = nn.Linear(in_features = 128, out_features = 64)\n# or\nlinear = nn.Linear(128, 64)\n```\n\nThe input tensor is normally _[batch_size, input_features]_ where batch size is the number of samples processed in a single forward pass. The weight matrix has the shape _[out_features, in_features]_ and it is initialized randomly by default. The model will learn appopriate values of these weights during training. The bias vector has the shape _[out_features]_. This is also updated during training.","metadata":{}},{"cell_type":"markdown","source":"## Convolution Layer\n\nA convolution layer is in the heart of convolution neural networks. its goal is to extract meaningful features from input data by applying convolutio operations. These layers performs a critical mathematical operation known as a __convolution__. \n\nThis process using filters known as __kernel__ that traverse through the input image to learn complex visual patterns.\n\n### convolution operation\n\nThis is a linear operation between two functions commonly applied to signals images or any structured data.\n\n![2D Convolution Animation](https://upload.wikimedia.org/wikipedia/commons/1/19/2D_Convolution_Animation.gif)\n\nwhen you have the input matrix, and the kernel, you overlay the kernel on the input matriz, do an elementwise multiplication (multiply each overlapping element), and then sum up the results it forms the first element in the output matrix, the slide the kernel based on the stride and and repeat until you get an output matrix.\n\nThis output matrix is known as a __feature map__.","metadata":{}},{"cell_type":"markdown","source":"### channels\n\nEach channel represents a color, each pixel consists of three channels if it is an RGB. An RGB image can be described as _W x H x c_. Gray scale images have only one channel. ","metadata":{}},{"cell_type":"markdown","source":"### Parameters\n\n1. Filter/Kernel size\n2. Stride\n3. Padding\n4. Output channels\n5. in channels\n\n```python\nconvLayer = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2, stride = 1, padding = 0)\n```\n\n#### Filter/kernel size\n\nThese are the learnable weight matrices. Each kernel extracts specific features like edges textures or patterns. for a filter size _k x K_ and input with c_in channels, total weights equates to _k x k x c_in_\n\n#### input channels\n\nrepresents the number of channels in the input data. for RGB it is three channels (Red, Green, Blue) and for grayscale it is just 1.\n\n#### Kernel size\n\nspatial size of the filter. Typically 3 x 3, 5 x 5, 7 x 7.\n\n#### Stride\n\ndetermines how much a filter move at each step. a stride of 1 means one pixel at a time. Larger strides reduces the size of the output feature map.\n\n#### padding\n\nadds zeros around the inputs to maintain its size after convolutions. \n\n#### Hyperparameters affecting parameters\n\n1. increasing output channels increases the number of filters and hence more learnable parameters\n2. increasing kernel size makes each filter larger\n3. ","metadata":{}},{"cell_type":"markdown","source":"## Pooling layers\n\nPooling, also known as subsampling or downsampling, is a technique used in CNNs to reduce the spatial dimensions of feature maps while retaining essential information\n\n1. MaxPooling\n2. AvePooling\n\n### MaxPooling\n\nTake the maximum value from each region of the feature map. It captures the most prominent feature in each region. \n\n### average pooling\n\nTake the average value from each region of the feature map\n\n### parameters\n\n1. __kernel size__  - determines the region to be pooled\n2. __stride__ - determines the step size of the pooling window, default is equal to the kernel size to ensure no overlapping regions\n3. __padding__ - add zeros around the input to control the output dimensions.\n\n```python\nmax_pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\navg_pool = nn.AvgPool2d(kernel_size = 2, stride = 2)\n```\n\npooling:\n\n1. reduces dimensions\n2. translation invariance - help the network recognize patterns regardless of small shifts input\n3. prevent overfitting","metadata":{}},{"cell_type":"markdown","source":"## Recurrent layers","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
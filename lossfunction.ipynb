{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LOSS FUNCTIONS\n\nThey are used to measure the difference between the model's predictions and the actual target values, it guides the optimizer to adjust weigths and improve performances. Pytorch has many builtin loss functions in `torch.nn` and also supports custom implementations.\n\nFor __regression losses__:\n\n1. Mean Squared Error Loss (MSE) (`nn.MSELoss`) - measures the squared difference between predictions and targets\n2. Mean Absolute Error Loss (`nn.L1Loss`) - measures the absolute difference between the predictions and targets\n\nFor __classification losses__:\n\n1. Cross-Entropy Loss(`nn.CrossEntropyLoss`) - it combines `nn.LogSoftmax` and `nn.NLLLoss` in a single class. typically used in a multi-class classification problem\n2. Binary Cross-Entropy Loss (`nn.BCELoss`) - used for binary classification problems with probabilities as predictions\n3. Binary Cross-Entropy with Logits Loss (`nn.BCEWithLogitsLoss`) - combines sigmoid layer and binary cross-entropy loss in one function","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\ncriterion = nn.MSELoss()\nloss = criterion(predicted, actual)\nprint(loss.item())","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}